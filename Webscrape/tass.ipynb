{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from http import client\n",
    "from json import load\n",
    "import pandas\n",
    "import json\n",
    "import requests\n",
    "import untangle\n",
    "import pymongo\n",
    "import os\n",
    "import ssl\n",
    "# from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_urls(url):\n",
    "    # Get the RSS feed\n",
    "    # MSNBC Wayback machine archive urls\n",
    "    urls = requests.get(url,headers={'User-Agent': 'Custom'}).text\n",
    "    parse_url = json.loads(urls) #parses the JSON from urls.\n",
    "    ## Extracts timestamp and original columns from urls and compiles a url list.\n",
    "    url_list = []\n",
    "    for i in range(1,len(parse_url)):\n",
    "        orig_url = parse_url[i][2]\n",
    "        tstamp = parse_url[i][1]\n",
    "        waylink = tstamp+'/'+orig_url\n",
    "        url_list.append(waylink)\n",
    "    ## Compiles final url pattern.\n",
    "    final_url_list= []\n",
    "    for url in url_list:\n",
    "        final_url = 'https://web.archive.org/web/'+url\n",
    "        final_url_list.append(final_url)\n",
    "\n",
    "    link_array = []\n",
    "    for link in final_url_list:\n",
    "        import ssl\n",
    "        if hasattr(ssl, '_create_unverified_context'):\n",
    "            ssl._create_default_https_context = ssl._create_unverified_context\n",
    "            \n",
    "        second_http =  link.index('/http')\n",
    "        link =  link[:second_http] + 'if_/' + link[second_http+1:]\n",
    "        \n",
    "        \n",
    "        print(link)\n",
    "        document = untangle.parse(link)\n",
    "        items = document.rss.channel.item\n",
    "        for item in items:\n",
    "            link = item.link.cdata.split('?')[0]\n",
    "            if link not in link_array:\n",
    "                link_array.append(link)\n",
    "    link_array.pop(0)\n",
    "    return link_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_article_data(article_url):\n",
    "    #article_html = requests.get(article_url).text\n",
    "    link = article_url.split(\"https://\")\n",
    "    article_html = requests.get(f\"https://webcache.googleusercontent.com/search?q=cache%3Ahttp%3A//{link[1]}\",headers={'User-Agent': 'Custom'}).text\n",
    "\n",
    "        \n",
    "    # Parse the HTML\n",
    "    soup = BeautifulSoup(article_html, 'html.parser')\n",
    "    #title = soup.find(class_=\"css-1vkm6nb ehdk2mb0\").text\n",
    "    title = soup.find('title').text.strip('- TASS')\n",
    "    time = ''\n",
    "    #time = soup.find('meta',property=\"article:modified_time\")['content']\n",
    "    \n",
    "    body_list = []\n",
    "    for item in soup.find_all('div', class_=\"text-block\"):\n",
    "        body_list.append(item.text)\n",
    "    # body_list = body_list[0].split('\\n')\n",
    "        \n",
    "    article_data_obj = {\n",
    "        'title': title,\n",
    "        'posted_at': time,\n",
    "        'body': ' '.join(body_list)\n",
    "    }\n",
    "    \n",
    "    return article_data_obj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://web.archive.org/web/20220313005828if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220315040523if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220315085848if_/https://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220316034046if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220316052454if_/https://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220318035523if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220318035751if_/https://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220319232751if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220320064956if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220320065135if_/https://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220321040934if_/http://tass.com/rss/v2.xml\n",
      "https://web.archive.org/web/20220321080625if_/https://tass.com/rss/v2.xml\n"
     ]
    }
   ],
   "source": [
    "tass_rss_url = 'http://web.archive.org/cdx/search/cdx?url=tass.com/rss/v2.xml&from=20220313&to=20220321&output=json'\n",
    "articles = get_article_urls(tass_rss_url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'https://tass.com/science/1421121'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "articles[41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleList = []\n",
    "count = 0\n",
    "for article in articles:\n",
    "    print(count)\n",
    "    article_data_obj = get_article_data(article)\n",
    "    print(article_data_obj['title'])\n",
    "    articleJson = {\n",
    "            'title': article_data_obj['title'],\n",
    "            # 'posted_at': article_data_obj['posted_at'],\n",
    "            'body': article_data_obj['body'],\n",
    "        }\n",
    "\n",
    "    articleList.append(articleJson)\n",
    "    jsonString = json.dumps(articleList)\n",
    "    jsonFile = open(\"tass.json\", \"w\")\n",
    "    jsonFile.write(jsonString)\n",
    "    jsonFile.close()\n",
    "    count += 1\n",
    "df = pandas.read_json (r'tass.json')\n",
    "df.to_csv(r'tass.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "articleList = []\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "477\n",
      "mmonia leak occurs at chemical plant in Ukraine’s Sumy — local authorities -  Emergencies\n",
      "478\n",
      "Over 10 people hospitalized in Rostov-on-Don with burns after heating pipeline failure -  Emergencies\n"
     ]
    }
   ],
   "source": [
    "for article in range(count, len(articles)):\n",
    "    print(count)\n",
    "    article_data_obj = get_article_data(articles[article])\n",
    "    print(article_data_obj['title'])\n",
    "    if \"webcache.googleusercontent.com\" in article_data_obj['title']:\n",
    "        break\n",
    "    articleJson = {\n",
    "            'title': article_data_obj['title'],\n",
    "            # 'posted_at': article_data_obj['posted_at'],\n",
    "            'body': article_data_obj['body'],\n",
    "        }\n",
    "\n",
    "    articleList.append(articleJson)\n",
    "    count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 424"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonString = json.dumps(articleList)\n",
    "jsonFile = open(\"tass.json\", \"w\")\n",
    "jsonFile.write(jsonString)\n",
    "jsonFile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pandas.read_json (r'tass.json')\n",
    "df.to_csv(r'tass.csv', index = None)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "0e9793d42727781e2f5fdf91cc85593f327a2587237f01a3847e7531a12f4a6b"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
